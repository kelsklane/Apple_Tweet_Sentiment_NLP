{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b149c2c0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73fcca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Read in the data\n",
    "tweets = pd.read_csv('../data/clean_tweets.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ac6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter rt\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "def tweet_tokenizer(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scraping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of any tokens that represent if the tweet was retweeted\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    #Gets rid  of any punctuation that we don't want to keep\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_tweets(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scraping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of RT and @'s\n",
    "    doc = re.sub(r'RT ', '', doc)\n",
    "    doc = re.sub(r'@\\w+', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51c47997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tweet_text = tweets.tweet_text.apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fd164c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7ed2f645d14ab181ee76b3431fd9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=949.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce7a8a5c62b4a02890c13634e5d7890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=747.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb096d3a1e04f3bbcf8522b67871ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=498679497.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e05c1ded842e4aec202e6a3e4ac51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898822.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa19ff22dce4930b34c8e346744fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734bb758346c415b9b33983e9516302f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(model = \"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca5108",
   "metadata": {},
   "source": [
    "Generate predictions based on pre-trained model with no tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dda6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tweets.tweet_text.map(lambda x: sentiment_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85259758",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(tweets.tweet_text)):\n",
    "    if prediction[i][0]['label'][-1] == '1':\n",
    "        results.append(2) #Neutral\n",
    "    elif prediction[i][0]['label'][-1] == '2':\n",
    "        results.append(1) #Positive\n",
    "    else:\n",
    "        results.append(0) #Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0180189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566138540899042"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(tweets.label, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5ab54",
   "metadata": {},
   "source": [
    "For being untuned it does pretty good! The final original model has 62% accurracy, so a result of 57% isn't bad. Ideally taking this model as a based and tuning it to the dataset would output better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a653e",
   "metadata": {},
   "source": [
    "# Tuned Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['tweet_text'], truncation=True)\n",
    " \n",
    "tokenized_train = X_train.map(preprocess_function, batched=True)\n",
    "tokenized_test = X_test.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels = 3)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"apple-sentiment-model-play-data\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=True,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
