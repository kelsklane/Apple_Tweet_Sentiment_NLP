{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b149c2c0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fcca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Read in the data\n",
    "tweets = pd.read_csv('../data/clean_tweets.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ac6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter rt\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "def tweet_tokenizer(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scraping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of any tokens that represent if the tweet was retweeted\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    #Gets rid  of any punctuation that we don't want to keep\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_tweets(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', 'http', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', 'http', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scraping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of @'s associated with a RT\n",
    "    doc = re.sub(r'RT @\\w+', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = re.sub(r'&quot;', '\"', doc)\n",
    "    doc = remove_junk(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c47997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tweet_text = tweets.tweet_text.apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fd164c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7ed2f645d14ab181ee76b3431fd9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=949.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce7a8a5c62b4a02890c13634e5d7890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=747.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb096d3a1e04f3bbcf8522b67871ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=498679497.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e05c1ded842e4aec202e6a3e4ac51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898822.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa19ff22dce4930b34c8e346744fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734bb758346c415b9b33983e9516302f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(model = \"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca5108",
   "metadata": {},
   "source": [
    "Generate predictions based on pre-trained model with no tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dda6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tweets.tweet_text.map(lambda x: sentiment_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85259758",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(tweets.tweet_text)):\n",
    "    if prediction[i][0]['label'][-1] == '1':\n",
    "        results.append(2) #Neutral\n",
    "    elif prediction[i][0]['label'][-1] == '2':\n",
    "        results.append(1) #Positive\n",
    "    else:\n",
    "        results.append(0) #Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0180189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566138540899042"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(tweets.label, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5ab54",
   "metadata": {},
   "source": [
    "For being untuned it does pretty good! The final original model has 62% accurracy, so a result of 57% isn't bad. Ideally taking this model as a based and tuning it to the dataset would output better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a653e",
   "metadata": {},
   "source": [
    "# Fine Tuned Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d54af",
   "metadata": {},
   "source": [
    "Preprocess the text and tokenize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77fe09d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e7a5cbd264492291a341a9f2d793fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "def preprocess(tweet):\n",
    "    new_tweet = []\n",
    "    for t in tweet.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_tweet.append(t)\n",
    "    tweet_clean = \" \".join(new_tweet)\n",
    "    return tweet_clean.strip()\n",
    "\n",
    "tweets_only = tweets[['tweet_text', 'label']]\n",
    "tweets_only.tweet_text = tweets_only.tweet_text.apply(preprocess)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['tweet_text'], return_tensors='pt')\n",
    "\n",
    "df = Dataset.from_pandas(tweets_only)\n",
    "tokenized_datasets = df.map(tokenize_function)\n",
    "split_df = train_test_split(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51c8ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/config.json from cache at /Users/kelseylane/.cache/huggingface/transformers/7dd97280b5338fb674b5372829a05a1aaaa76f9f2fa71c36199f2ce1ee1104a0.4c7ca95b4fd82b8bbe94fde253f5f82e5a4eedefe6f86f6fa79efc903d6cfe60\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /Users/kelseylane/.cache/huggingface/transformers/11065c9c045391e7a6b2bb2451b862074866aeddabaece3ef767540b48247a1c.a8b67614ee564f9fefd65a3a42566038ccf3e86668cb888d8ae05ec670696ba7\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels = 3)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ed0d66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "training_args = TrainingArguments(\"test_trainer\")\n",
    " \n",
    "trainer = Trainer(model = model, \n",
    "                  args = training_args, \n",
    "                  train_dataset = split_df[0], \n",
    "                  eval_dataset = split_df[1],\n",
    "                 compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63f225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
